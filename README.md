# My Web Scraper :cloud:  
#### Python scripts to scrape Covid-19 cases data from [COVID-19 Central @ The U](https://coronavirus.utah.edu)   
The purpose of this is to keep track of the timeline, daily increases/decreases in the data they provide.  
 

* Scheduled scrape time: Daily at 12:30PM, 2:30PM and 6:00PM in MDT. :penguin:
* Scheduled data cleaning/processing time: Daily at 6:01PM in MDT. :blowfish:

#### Visit folder ```data/``` for the csv files  
* [Un-processed data from the covid cases panel](https://github.com/baohuy251210/Collector/blob/master/data/uofucovidinit_timeline.csv)  
* [Processed cumulative case counts On Campus since 8/15/20](https://github.com/baohuy251210/Collector/blob/master/data/cases_timeline.csv)  

*Since there's not many details, this is all the scripts can get right now.* 

#### Update 1 (9/3/2020):
* Implemented a better case counts data and a cleaner to process it everyday. 
* Old files like the weekly counts `uofucovid_timeline.csv` and old version `log.txt` are in folder `archive` 
